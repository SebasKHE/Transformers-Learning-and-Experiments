{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine-Tuning Transformers with PyTorch and Hugging Face**\n",
    "\n",
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project aims to introduce you to the process of loading and fine-tuning pretrained large language models (LLMs) \n",
    "\n",
    "You will learn how to implement the training loop of a model using pytorch to tune a model on task-specific data, as well as fine-tuning a model on task-specific data using the SFTTrainer module from Hugging Face. Finally, you will learn how to evaluate the performance of the fine-tuned models.\n",
    "\n",
    "By the end of this project, you will have a solid understanding of how to leverage pretrained LLMs and fine-tune them for your specific use cases, empowering you to create powerful and customized natural language processing solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Supervised-Fine-tuning-with-PyTorch)\">Supervised Fine-tuning with Pytorch</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Dataset-preparations\">Dataset preparations</a></li>\n",
    "            <li><a href=\"#Train-the-model\">Train the model</a></li>\n",
    "            <li><a href=\"#Evaluate\">Evaluate</a></li>\n",
    "            <li><a href=\"#Loading-the-saved-model\">Loading the saved model</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise:-Training-a-conversational-model-using-SFTTrainer\">Exercise: Training a conversational model using SFTTrainer</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    " - Load pretrained LLMs from Hugging Face and make inferences\n",
    " - Fine-tune a model on task-specific data using the SFTTrainer module from Hugging Face\n",
    " - Load a SFTTrainer pretrained model and make comparisons\n",
    " - Evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\n",
      "  Downloading pmdarima-2.0.4-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting joblib>=0.11 (from pmdarima)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima)\n",
      "  Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting numpy>=1.21.2 (from pmdarima)\n",
      "  Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pandas>=0.19 (from pmdarima)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn>=0.22 (from pmdarima)\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy>=1.3.2 (from pmdarima)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting statsmodels>=0.13.2 (from pmdarima)\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting urllib3 (from pmdarima)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting setuptools!=50.0.0,>=38.6.0 (from pmdarima)\n",
      "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=0.19->pmdarima)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=0.19->pmdarima)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.22->pmdarima)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels>=0.13.2->pmdarima)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
      "Downloading pmdarima-2.0.4-cp312-cp312-win_amd64.whl (625 kB)\n",
      "   ---------------------------------------- 0.0/625.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 625.1/625.1 kB 11.7 MB/s eta 0:00:00\n",
      "Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 79.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.6/12.6 MB 60.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 56.7 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.5/11.5 MB 103.1 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.1/11.1 MB 63.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.7/44.5 MB 21.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.9/44.5 MB 21.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.5/44.5 MB 18.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 19.1/44.5 MB 18.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 23.1/44.5 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 27.5/44.5 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.2/44.5 MB 19.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.2/44.5 MB 20.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/44.5 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 20.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 22.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.8 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 21.9 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, threadpoolctl, setuptools, numpy, joblib, Cython, scipy, patsy, pandas, statsmodels, scikit-learn, pmdarima\n",
      "Successfully installed Cython-3.0.11 joblib-1.4.2 numpy-2.2.0 pandas-2.2.3 patsy-1.0.1 pmdarima-2.0.4 pytz-2024.2 scikit-learn-1.6.0 scipy-1.14.1 setuptools-75.6.0 statsmodels-0.14.4 threadpoolctl-3.5.0 tzdata-2024.2 urllib3-2.2.3\n",
      "Collecting pmdarima==2.0.2\n",
      "  Downloading pmdarima-2.0.2.tar.gz (630 kB)\n",
      "     ---------------------------------------- 0.0/630.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 630.8/630.8 kB 24.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (1.4.2)\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (3.0.11)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (2.2.0)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (1.14.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (0.14.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (2.2.3)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pmdarima==2.0.2) (75.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas>=0.19->pmdarima==2.0.2) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from scikit-learn>=0.22->pmdarima==2.0.2) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from statsmodels>=0.13.2->pmdarima==2.0.2) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima==2.0.2) (1.17.0)\n",
      "Building wheels for collected packages: pmdarima\n",
      "  Building wheel for pmdarima (pyproject.toml): started\n",
      "  Building wheel for pmdarima (pyproject.toml): finished with status 'error'\n",
      "Failed to build pmdarima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pmdarima (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [41 lines of output]\n",
      "      <string>:15: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "      Partial import of pmdarima during the build process.\n",
      "      \n",
      "      Requirements: ['joblib>=0.11\\nCython>=0.29,!=0.29.18,!=0.29.31\\nnumpy>=1.21.2\\npandas>=0.19\\nscikit-learn>=0.22\\nscipy>=1.3.2\\nstatsmodels>=0.13.2\\nurllib3\\nsetuptools>=38.6.0,!=50.0.0\\n']\n",
      "      Adding extra setuptools args\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 190, in check_package_status\n",
      "        File \"D:\\Python\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
      "      ModuleNotFoundError: No module named 'numpy'\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 251, in build_wheel\n",
      "          return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Genia\\AppData\\Local\\Temp\\pip-build-env-0w5cp90a\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 438, in build_wheel\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Genia\\AppData\\Local\\Temp\\pip-build-env-0w5cp90a\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 426, in _build\n",
      "          return self._build_with_temp_dir(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Genia\\AppData\\Local\\Temp\\pip-build-env-0w5cp90a\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 407, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Genia\\AppData\\Local\\Temp\\pip-build-env-0w5cp90a\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\Genia\\AppData\\Local\\Temp\\pip-build-env-0w5cp90a\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 340, in <module>\n",
      "        File \"<string>\", line 327, in do_setup\n",
      "        File \"<string>\", line 210, in check_package_status\n",
      "      ImportError: numpy is not installed.\n",
      "      pmdarima requires numpy >= 1.16.\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pmdarima\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pmdarima)\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "!pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118\n",
    "# - Update a specific package\n",
    "!pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "!pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.42.1\n",
      "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers==4.42.1)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.42.1)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0,>=1.17 (from transformers==4.42.1)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers==4.42.1) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.42.1)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.42.1)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers==4.42.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.1)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.42.1)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.42.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from tqdm>=4.27->transformers==4.42.1) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers==4.42.1)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.42.1)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->transformers==4.42.1) (2.2.3)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.42.1)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading transformers-4.42.1-py3-none-any.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.3 MB 49.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.1/9.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.4/9.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  15.5/15.5 MB 88.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 75.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.0\n",
      "    Uninstalling numpy-2.2.0:\n",
      "      Successfully uninstalled numpy-2.2.0\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.27.0 idna-3.10 numpy-1.26.4 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.67.1 transformers-4.42.1 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1) (4.12.2)\n",
      "Collecting sympy (from torch==2.3.1)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.3.1)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch==2.3.1)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1) (2024.10.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch==2.3.1)\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1)\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1)\n",
      "  Downloading tbb-2021.13.1-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.3.1)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.1-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "   ---------------------------------------- 0.0/159.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 15.7/159.7 MB 82.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 35.1/159.7 MB 89.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 39.3/159.7 MB 67.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 43.0/159.7 MB 53.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 46.4/159.7 MB 46.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 50.1/159.7 MB 41.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 54.3/159.7 MB 38.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 58.5/159.7 MB 36.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 62.9/159.7 MB 34.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 67.9/159.7 MB 33.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 72.4/159.7 MB 32.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 76.0/159.7 MB 31.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 80.0/159.7 MB 30.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 84.1/159.7 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 88.6/159.7 MB 29.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 92.0/159.7 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 95.7/159.7 MB 27.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 99.4/159.7 MB 27.2 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 103.3/159.7 MB 26.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 107.7/159.7 MB 26.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 112.2/159.7 MB 26.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 115.9/159.7 MB 26.0 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 119.3/159.7 MB 25.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 122.9/159.7 MB 25.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 127.1/159.7 MB 25.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 131.3/159.7 MB 24.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 136.1/159.7 MB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 140.8/159.7 MB 24.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 144.2/159.7 MB 24.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 147.8/159.7 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 152.0/159.7 MB 24.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.5/159.7 MB 24.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.6/159.7 MB 24.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 159.7/159.7 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.0/228.5 MB 23.2 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 10.0/228.5 MB 23.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 15.2/228.5 MB 24.5 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 19.9/228.5 MB 24.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 24.1/228.5 MB 23.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 27.8/228.5 MB 22.9 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 31.2/228.5 MB 21.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 34.3/228.5 MB 21.2 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 38.3/228.5 MB 20.8 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 41.9/228.5 MB 20.5 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 46.1/228.5 MB 20.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 50.6/228.5 MB 20.6 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 55.3/228.5 MB 20.8 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 60.3/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 65.0/228.5 MB 21.4 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 69.2/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 73.4/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 77.6/228.5 MB 21.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 81.5/228.5 MB 21.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 84.9/228.5 MB 20.8 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 88.6/228.5 MB 20.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 92.3/228.5 MB 20.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 96.5/228.5 MB 20.6 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 100.9/228.5 MB 20.6 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 105.6/228.5 MB 20.8 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 110.4/228.5 MB 20.8 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 114.3/228.5 MB 20.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 118.0/228.5 MB 20.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 122.2/228.5 MB 20.7 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 126.4/228.5 MB 20.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 130.8/228.5 MB 20.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 134.2/228.5 MB 20.7 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 137.9/228.5 MB 20.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 141.8/228.5 MB 20.5 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 146.0/228.5 MB 20.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 150.5/228.5 MB 20.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 155.2/228.5 MB 20.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 159.4/228.5 MB 20.7 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 163.1/228.5 MB 20.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 167.0/228.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 170.9/228.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 175.4/228.5 MB 20.6 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 180.1/228.5 MB 20.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 183.5/228.5 MB 20.5 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 187.4/228.5 MB 20.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 191.4/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 195.6/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 200.3/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 205.0/228.5 MB 20.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 208.9/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 212.3/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 216.3/228.5 MB 20.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 220.7/228.5 MB 20.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  225.4/228.5 MB 20.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.3/228.5 MB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 228.5/228.5 MB 20.1 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.5/3.5 MB 20.9 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.13.1-py3-none-win_amd64.whl (286 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 5.2/6.2 MB 24.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 22.4 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 intel-openmp-2021.4.0 jinja2-3.1.4 mkl-2021.4.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.3 tbb-2021.13.1 torch-2.3.1\n",
      "Collecting torchmetrics==1.4.0.post0\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torchmetrics==1.4.0.post0) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torchmetrics==1.4.0.post0) (24.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torchmetrics==1.4.0.post0) (2.3.1)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.4.0.post0)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics==1.4.0.post0) (75.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics==1.4.0.post0) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (2024.10.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->torchmetrics==1.4.0.post0) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics==1.4.0.post0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->torchmetrics==1.4.0.post0) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from jinja2->torch>=1.10.0->torchmetrics==1.4.0.post0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from sympy->torch>=1.10.0->torchmetrics==1.4.0.post0) (1.3.0)\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "   ---------------------------------------- 0.0/868.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 868.8/868.8 kB 40.4 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.4.0.post0\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (1.26.4)\n",
      "Collecting peft==0.11.1\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (2.3.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (4.42.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.11.1)\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from peft==0.11.1) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2024.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.13.0->peft==0.11.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.13.0->peft==0.11.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.13.0->peft==0.11.1) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from tqdm->peft==0.11.1) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers->peft==0.11.1) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers->peft==0.11.1) (0.19.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft==0.11.1) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft==0.11.1) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.2.1 peft-0.11.1\n",
      "Collecting evaluate==0.4.2\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate==0.4.2)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (1.26.4)\n",
      "Collecting dill (from evaluate==0.4.2)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (4.67.1)\n",
      "Collecting xxhash (from evaluate==0.4.2)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from evaluate==0.4.2)\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.2) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from evaluate==0.4.2) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets>=2.0.0->evaluate==0.4.2) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill (from evaluate==0.4.2)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate==0.4.2)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate==0.4.2)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading aiohttp-3.11.10-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets>=2.0.0->evaluate==0.4.2) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests>=2.19.0->evaluate==0.4.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests>=2.19.0->evaluate==0.4.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests>=2.19.0->evaluate==0.4.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests>=2.19.0->evaluate==0.4.2) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from tqdm>=4.62.1->evaluate==0.4.2) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->evaluate==0.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->evaluate==0.4.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->evaluate==0.4.2) (2024.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.0.0->evaluate==0.4.2)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.2) (1.17.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.11.10-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 10.7/25.1 MB 51.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 69.2 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.2 attrs-24.3.0 datasets-3.2.0 dill-0.3.8 evaluate-0.4.2 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 xxhash-3.5.0 yarl-1.18.3\n",
      "Collecting accelerate==0.31.0\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (2.3.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate==0.31.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (2024.9.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.31.0) (2021.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub->accelerate==0.31.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from huggingface-hub->accelerate==0.31.0) (4.67.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.31.0) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.31.0) (2021.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.31.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.31.0) (2024.12.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.2.1\n",
      "    Uninstalling accelerate-1.2.1:\n",
      "      Successfully uninstalled accelerate-1.2.1\n",
      "Successfully installed accelerate-0.31.0\n",
      "Collecting torchvision==0.18.1\n",
      "  Downloading torchvision-0.18.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torchvision==0.18.1) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torchvision==0.18.1) (2.3.1)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.18.1)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (2024.9.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch==2.3.1->torchvision==0.18.1) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision==0.18.1) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision==0.18.1) (2021.13.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from jinja2->torch==2.3.1->torchvision==0.18.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from sympy->torch==2.3.1->torchvision==0.18.1) (1.3.0)\n",
      "Downloading torchvision-0.18.1-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 28.8 MB/s eta 0:00:00\n",
      "Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 72.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-11.0.0 torchvision-0.18.1\n",
      "Collecting trl==0.9.4\n",
      "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from trl==0.9.4) (2.3.1)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from trl==0.9.4) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from trl==0.9.4) (1.26.4)\n",
      "Requirement already satisfied: accelerate in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from trl==0.9.4) (0.31.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from trl==0.9.4) (3.2.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.9.4)\n",
      "  Downloading tyro-0.9.2-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (2024.9.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from torch>=1.4.0->trl==0.9.4) (2021.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from transformers>=4.31.0->trl==0.9.4) (4.67.1)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from tyro>=0.5.11->trl==0.9.4) (0.4.6)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from accelerate->trl==0.9.4) (6.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from datasets->trl==0.9.4) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from aiohttp->datasets->trl==0.9.4) (1.18.3)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl==0.9.4) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.4.0->trl==0.9.4) (2021.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->transformers>=4.31.0->trl==0.9.4) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->transformers>=4.31.0->trl==0.9.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->transformers>=4.31.0->trl==0.9.4) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from requests->transformers>=4.31.0->trl==0.9.4) (2024.12.14)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.4) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from jinja2->torch>=1.4.0->trl==0.9.4) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->datasets->trl==0.9.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->datasets->trl==0.9.4) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from pandas->datasets->trl==0.9.4) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from sympy->torch>=1.4.0->trl==0.9.4) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.4)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.4) (1.17.0)\n",
      "Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
      "Downloading tyro-0.9.2-py3-none-any.whl (112 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typeguard, shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.16 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.9.4 shtab-1.7.1 trl-0.9.4 typeguard-4.4.1 tyro-0.9.2\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-3.20.3\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\genia\\onedrive\\escritorio\\aprendiendo de ia\\pe - generative ai engineering with llms\\4. generative ai engineering and fine-tuning transformers\\mienv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.0/8.0 MB 55.2 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 60.8 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.7 matplotlib-3.10.0 pyparsing-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.42.1\n",
    "!pip install datasets # 2.20.0\n",
    "!pip install portalocker>=2.0.0\n",
    "!pip install torch==2.3.1\n",
    "!pip install torchmetrics==1.4.0.post0\n",
    "!pip install numpy==1.26.4\n",
    "!pip install peft==0.11.1\n",
    "!pip install evaluate==0.4.2\n",
    "!pip install -q bitsandbytes==0.43.1\n",
    "!pip install accelerate==0.31.0\n",
    "!pip install torchvision==0.18.1\n",
    "\n",
    "\n",
    "!pip install trl==0.9.4\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n",
    "* Note: If you get an error after running the cell below, try restarting the Kernel, as some packages need a restart to be effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig,SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import datasets\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-tuning with Pytorch\n",
    "\n",
    "Fine-tuning Transformers, specifically BERT (Bidirectional Encoder Representations from Transformers), refers to the process of training a pretrained BERT model on a specific downstream task. BERT is an encoder-only language model that has been pretrained on a large corpus of text to learn contextual representations of words.\n",
    "\n",
    "Fine-tuning BERT involves taking the pretrained model and further training it on a task-specific dataset, such as sentiment analysis or question answering. During fine-tuning, the parameters of the pretrained BERT model are updated and adapted to the specifics of the target task.\n",
    "\n",
    "This process is important because it allows you to leverage the knowledge and language understanding captured by BERT and apply it to different tasks. By fine-tuning BERT, you can benefit from its contextual understanding of language and transfer that knowledge to specific domain-specific or task-specific problems. Fine-tuning enables BERT to learn from a smaller labeled dataset and generalize well to unseen examples, making it a powerful tool for various natural language processing tasks. It helps to bridge the gap between pretraining on a large corpus and the specific requirements of downstream applications, ultimately improving the performance and effectiveness of models in various real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparations\n",
    "\n",
    "The Yelp review dataset is a widely used dataset in natural language processing (NLP) and sentiment analysis research. It consists of user reviews and accompanying metadata from the Yelp platform, which is a popular online platform for reviewing and rating local businesses such as restaurants, hotels, and shops.\n",
    "\n",
    "The dataset includes 6,990,280 reviews written by Yelp users, covering a wide range of businesses and locations. Each review typically contains the text of the review itself alongwith the star rating given by the user (ranging from 1 to 5).\n",
    "\n",
    "Our aim in this lab, is to fine-tune a pretrained BERT model to predict the ratings from reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the yelp_review data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 650000/650000 [00:00<00:00, 876072.31 examples/s]\n",
      "Generating test split: 100%|██████████| 50000/50000 [00:00<00:00, 847402.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a sample record of the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the label is the key of the class label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is also the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select a portion of data to decrease the training time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n",
    "dataset[\"test\"] = dataset[\"test\"].select([i for i in range(200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two data fields:\n",
    "- label: the label for the review\n",
    "- text: a string containing the body of the user review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data\n",
    "\n",
    "The next step is to load a BERT tokenizer to tokenize, pad and truncate reviews to handle variable-length sequences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4508.06 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3635.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a tokenizer using the BERT base cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Define a function to tokenize examples\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # Apply padding to ensure all sequences have the same length\n",
    "    # Apply truncation to limit the maximum sequence length\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Apply the tokenize function to the dataset in batches\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in each element of tokenized_datasets are 'label', 'text', 'input_ids', 'token_type_ids', and 'attention_mask'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, let's use the map method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is built on the PyTorch framework, it is crucial to prepare the dataset in a format that PyTorch can readily process. Follow these steps to ensure compatibility:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the text column because the model does not accept raw text as an input\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# Rename the label column to labels because the model expects the argument to be named labels\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set the format of the dataset to return PyTorch tensors instead of lists\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result is a set of tensors with the keys as:  'labels', 'input_ids', 'token_type_ids', 'attention_mask'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a DataLoader for train and test datasets so you can iterate over batches of data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training data loader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=2)\n",
    "\n",
    "# Create an evaluation data loader\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re ready to start training your model, now!\n",
    "In this section, you will learn to create the training loop from scratch without the help of the Hugging Face trainer class.\n",
    "In the MLM task, you utilized the Hugging Face trainer module. Now, you will develop the trainer yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pretrained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll load a pretrained classification model with 5 classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a sequence classification model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and learning rate schedule\n",
    "\n",
    "Let's create an optimizer and learning rate scheduler to fine-tune the model. You can use the AdamW optimizer from PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Calculate the total number of training steps\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda current_step: (1 - current_step / num_training_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Check if CUDA is available and, then set the device accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "You are ready to fine-tune the model. To keep track of training progress, let's use the \"tqdm\" library to add a progress bar over the number of training steps.\n",
    "The train_model function trains a model using a set of training data provided through a dataloader. It begins by setting up a progress bar to help monitor the training progress visually. The model is switched to training mode, which is necessary for certain model behaviors like dropout to work correctly during training. The function processes the data in batches for each epoch, which involves several steps for each batch: transferring the data to the correct device (like a GPU), running the data through the model to get outputs and calculate loss, updating the model's parameters using the calculated gradients, adjusting the learning rate, and clearing the old gradients. These steps are repeated for each batch of data, and the progress bar is updated accordingly to reflect the progress. Once all epochs are completed, the trained model is saved to be used later.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,tr_dataloader):\n",
    "\n",
    "    # Create a progress bar to track the training progress\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    tr_losses=[]\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0 \n",
    "        # Iterate over the training data batches\n",
    "        for batch in tr_dataloader:\n",
    "            # Move the batch to the appropriate device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**batch)\n",
    "            # Compute the loss\n",
    "            loss = outputs.loss\n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            # Update the learning rate scheduler\n",
    "            lr_scheduler.step()\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "        tr_losses.append(total_loss/len(tr_dataloader))\n",
    "    #plot loss\n",
    "    plt.plot(tr_losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "The evaluate_model function works similarly to the train_model function but is used for evaluating the model's performance instead of training it. It uses a dataloader to process data in batches, setting the model to evaluation mode to ensure accuracy in measurements and disabling gradient calculations since it's not training. The function calculates predictions for each batch, updates an accuracy metric, and finally, prints the overall accuracy after processing all batches.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, evl_dataloader):\n",
    "    # Create an instance of the Accuracy metric for multiclass classification with 5 classes\n",
    "    metric = Accuracy(task=\"multiclass\", num_classes=5).to(device)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the evaluation data batches\n",
    "        for batch in evl_dataloader:\n",
    "            # Move the batch to the appropriate device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Get the predicted class labels\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Accumulate the predictions and labels for the metric\n",
    "            metric(predictions, batch[\"labels\"])\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = metric.compute()\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Accuracy:\", accuracy.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train the model. This process will take a long time, and it is highly recommended that you do this only if you have the required resources. Please uncomment the following code to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:09<3:17:51,  2.38s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtr_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, tr_dataloader)\u001b[0m\n\u001b[0;32m     15\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model,tr_dataloader=train_dataloader)\n",
    "\n",
    "torch.save(model, 'my_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss_gpt.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HausLW2F_w30s1UK0zj7mQ/training-loss-BERT-Classification.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to learn how to tune a more complex model that can generate conversations between a human and an assistant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load the model that you trained for 10 epochs, go ahead and uncomment the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/wFhKpkBMSgjmZKRSyayvsQ/bert-classification-model.pt'\n",
    "model.load_state_dict(torch.load('bert_classification_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now evaluate the model. Please note that this process will take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26499998569488525\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to learn to tune a more complex model that can generate conversations between a human and an assistant using SFTtrainer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Training a conversational model using SFTTrainer\n",
    "\n",
    "The SFTTrainer from the trl (Transformers Reinforcement Learning) library is a tool used for supervised fine-tuning of language models. It helps refine pre-trained models using specific datasets to enhance their performance on targeted tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Explore how fine-tuning a decoder transformer using a specific dataset affects the quality of the generated responses in a question-answering task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1- Load the train split of \"timdettmers/openassistant-guanaco\" dataset from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 9846/9846 [00:00<00:00, 307661.66 examples/s]\n",
      "Generating test split: 100%|██████████| 518/518 [00:00<00:00, 73992.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write your code here\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# load dataset\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "dataset[0]\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2- Load the pretrained causal model \"facebook/opt-350m\" along with its tokenizer from Hugging Face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# load Hugging Face pretrained model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3- Create instruction and response templates based on the train dataset format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4- Create a collator to curate data in the appropriate shape for training using \"DataCollatorForCompletionOnlyLM\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5- Create an SFTTrainer object and pass the model as well as the dataset and collator: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [07:10<149:25:39, 107.67s/it]\n",
      "Map: 100%|██████████| 9846/9846 [00:02<00:00, 4136.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=10,\n",
    "    #learning_rate=2e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size\n",
    "    per_device_eval_batch_size=2,  # Reduce batch size\n",
    "    #gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/tmp\",\n",
    "    num_train_epochs=10,\n",
    "    #learning_rate=2e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=2,  # Reduce batch size\n",
    "    per_device_eval_batch_size=2,  # Reduce batch size\n",
    "    #gradient_accumulation_steps=4,  # Accumulate gradients\n",
    "    max_seq_length=1024,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6- Prompt the pretrained model with a specific question: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n",
      "\n",
      "The term \"monopsony\" is used in the context of the \"mono\" (mono-economy) model. The term \"mono-economy\" is used in the context of the \"mono-economy\" model. The term \"mono-economy\" is used in the context of\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,max_new_tokens=70)\n",
    "print(pipe('''Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,max_new_tokens=70)\n",
    "print(pipe('''Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model is barely aware of what \"monopsony\" is in the context of economics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6A (Optional)- Train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/49230 [00:19<268:38:25, 19.65s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Write your code here\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2274\u001b[0m ):\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1118\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1115\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1118\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   1132\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:884\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    874\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    875\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    876\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         use_cache,\n\u001b[0;32m    882\u001b[0m     )\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 884\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:525\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    522\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    533\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Genia\\OneDrive\\Escritorio\\Aprendiendo de IA\\PE - Generative AI Engineering with LLMs\\4. Generative AI Engineering and Fine-Tuning Transformers\\mienv\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:192\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    189\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mproj_shape)\n\u001b[0;32m    191\u001b[0m src_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## Write your code here\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you do not have enough resources to run the training, load the tuned model we provide here: \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Assistant_model.pt\":\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 6B- Load the tuned model: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write your code here\n",
    "model.load_state_dict(torch.load('Assistant_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Assistant_model.pt'\n",
    "model.load_state_dict(torch.load('Assistant_model.pt',map_location=torch.device('cpu')))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7- Check how the tuned model performs in answering the same specialized question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n",
      "\n",
      "The term \"monopsony\" in economics has been used in the context of the trade and economic system in the United States. The term \"monopsony\" has been used to refer to a situation where a large number of companies are owned by a single company, which can be seen as a monopsony in the sense\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,max_new_tokens=70)\n",
    "print(pipe('''Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,max_new_tokens=70)\n",
    "print(pipe('''Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fateme Akbari](https://author.skills.network/instructors/fateme_akbari) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "813811c42412546cc91d90735ea8e30caab5e8398335372a899df8c59fe00713"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
